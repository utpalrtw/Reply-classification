# -*- coding: utf-8 -*-
"""Reply classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b1zqnFnJql0umdgPKgiQQ98VVP5H4XC7

##**Reply Classification Model**
Import Lib
"""

import numpy as np
import pandas as pd
import re

df = pd.read_csv("/content/reply_classification_dataset.csv")

df.sample(5)

df.info()

df.shape

# Check for any duplicate data
df.duplicated().sum()

df['reply'].unique()

df.drop_duplicates(inplace=True)

df.head()

df.info()

df.info()

"""### EDA"""

df['label'].value_counts()

def clean_text(s):
    if not isinstance(s, str):
        return ""
    s = s.lower().strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^a-z0-9.,!?\s']", " ", s)
    return s

df['reply'] = df['reply'].apply(clean_text)

df['label'] = df['label'].apply(clean_text)

df.sample(5)

import random
import nltk
import pandas as pd
from nltk.corpus import wordnet

# Download WordNet data
nltk.download('wordnet')
nltk.download('omw-1.4')

def synonym_replacement(text, n=1):
    words = text.split()
    new_words = words.copy()
    random_word_list = list(set([w for w in words if len(wordnet.synsets(w)) > 0]))
    if not random_word_list:
        return text
    word_to_replace = random.choice(random_word_list)
    synonyms = wordnet.synsets(word_to_replace)
    lemmas = list(set([lemma.name() for s in synonyms for lemma in s.lemmas()]))
    lemmas = [w for w in lemmas if w.lower() != word_to_replace.lower()]
    if lemmas:
        new_words = [random.choice(lemmas) if w == word_to_replace else w for w in words]
    return " ".join(new_words)


# Generate augmented data
augmented_rows = []
for i, row in df.iterrows():
    new_text = synonym_replacement(row["reply"])
    augmented_rows.append({"reply": new_text, "label": row["label"]})

df_augmented = pd.DataFrame(augmented_rows)

# Combine original + augmented
df_final = pd.concat([df, df_augmented], ignore_index=True)

df_final.sample(10)

df_final.info()

df_final['reply'].unique()

df_final['label'].value_counts()

#encoding Target column into 0, 1, 2 lable ( negative = 0, neutral = 1, positive = 2)
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df_final['label'] = encoder.fit_transform(df_final['label'])

df_final['label'].value_counts()

df_final.head()

import matplotlib.pyplot as plt
plt.pie(df_final['label'].value_counts(), labels=['positive','negative','neutral'], autopct="%0.2f")
plt.title('Target Distribution')
plt.show()

"""###Data Preprocessing
* Lower case
* Tokenization
* Removing special characters
* Removing stop words and punctuation
* Stemming




"""

import nltk

nltk.download('punkt')

from nltk.stem.porter import PorterStemmer
import string
string.punctuation
ps = PorterStemmer()

nltk.download('stopwords')

from nltk.corpus import stopwords

def transform_text(text):
  text = text.lower()
  text = nltk.word_tokenize(text)

  y = []
  for i in text:
    if i.isalnum():
      y.append(i)
  text = y[:]
  y.clear()



  for i in text:
    if i not in string.punctuation:
       y.append(i)
  text = y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))

  return " ".join(y)

import nltk
nltk.download('punkt_tab')

df_final['transform_reply'] = df_final['reply'].apply(transform_text)

df_final.sample(10)

df_final["transform_reply"] = df_final["transform_reply"].fillna(df_final["reply"])

from wordcloud import WordCloud
wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')

negative_wc = wc.generate(df_final[df_final['label']==0]['transform_reply'].str.cat(sep=" "))

plt.figure(figsize=(10,5))
plt.imshow(negative_wc)  #wordcloud for negative messages

neutral_wc = wc.generate(df_final[df_final['label']==1]['transform_reply'].str.cat(sep=" "))

plt.figure(figsize=(10,5))
plt.imshow(neutral_wc)  #wordcloud for neutral messages

positive_wc = wc.generate(df_final[df_final['label']==2]['transform_reply'].str.cat(sep=" "))

plt.figure(figsize=(10,5))
plt.imshow(positive_wc)  #wordcloud for positive messages



"""###Model Building"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()

X = cv.fit_transform(df_final['transform_reply']).toarray()

X.shape

y = df_final['label'].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
gnb = GaussianNB()
mnb = MultinomialNB()

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score

gnb.fit(X_train, y_train)
y_pred1 = gnb.predict(X_test)
print(confusion_matrix(y_test, y_pred1))
print("accuracy score :", accuracy_score(y_test, y_pred1))
print("precision_score :",precision_score(y_test, y_pred1, average='weighted'))

mnb.fit(X_train, y_train)
y_pred2 = mnb.predict(X_test)
print(confusion_matrix(y_test, y_pred2))
print("accuracy score :", accuracy_score(y_test, y_pred2))
print("precision_score :",precision_score(y_test, y_pred2, average='weighted'))

from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(max_features=3000)

X = tv.fit_transform(df_final['transform_reply']).toarray()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
gnb = GaussianNB()
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb = XGBClassifier(n_estimators=50,random_state=2)

clfs = {
    'SVC' : svc,
    'KN' : knc,
    'NB': mnb,
    'GB' : gnb,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'ETC': etc,
    'GBDT':gbdt,
    'xgb':xgb
}

def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred,average='weighted')

    return accuracy,precision

train_classifier(xgb,X_train,y_train,X_test,y_test)

accuracy_scores = []
precision_scores = []

for name,clf in clfs.items():

    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

performance_df

performance_df1 = pd.melt(performance_df, id_vars = "Algorithm")

performance_df1

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)

new_df = performance_df.merge(temp_df,on='Algorithm')

new_df_scaled = new_df.merge(temp_df,on='Algorithm')

new_df_scaled.merge(temp_df,on='Algorithm')

# Voting Classifier
svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)
gnb = GaussianNB()
mnb = MultinomialNB()
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
knc = KNeighborsClassifier()


from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(estimators=[('ETC', etc), ('GB', gnb), ('NB', mnb),('GBDT', gbdt),('KN' , knc)],voting='soft')

voting.fit(X_train,y_train)

y_pred = voting.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred, average='weighted'))

# Applying stacking
estimators=[('ETC', etc), ('GB', gnb), ('NB', mnb),('GBDT', gbdt),('KN' , knc)]
final_estimator=RandomForestClassifier()

from sklearn.ensemble import StackingClassifier
clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred,average='weighted'))

import pickle
pickle.dump(tv,open('vectorizer_tfidf.pkl','wb'))
pickle.dump(clf,open('clf_model.pkl','wb'))